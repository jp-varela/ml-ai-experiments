{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmLAqLDucO2Q"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nfpaiva/ml-ai-experiments/blob/main/notebooks/ai-act-chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# ü§ñüìö Talking to the EU AI Act: A Didactic RAG Demo\n",
    "\n",
    "This notebook demonstrates how to build a **simple Retrieval-Augmented Generation (RAG) system** to interact with large documents ‚Äî using the **EU Artificial Intelligence Act (AI Act)** as an example.\n",
    "\n",
    "As organizations move toward compliance with the AI Act and other regulatory frameworks, understanding how to **search, interpret, and explain** legal and technical documents becomes a critical skill.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ What You‚Äôll Learn and Explore\n",
    "\n",
    "- ‚úÖ How to load, chunk, and embed long documents (like the full 144-page AI Act PDF)\n",
    "- ‚úÖ How to build a question-answering system that **retrieves** relevant text and **generates** natural language answers\n",
    "- ‚úÖ How to apply **open-source models** like FLAN-T5-XL for domain-specific QA\n",
    "- ‚úÖ How prompt design, chunk size, and retrieval parameters impact results\n",
    "- ‚úÖ How RAG systems can help bridge the gap between:\n",
    "  - Legal documents  \n",
    "  - Business requirements  \n",
    "  - Technical artifacts (like model cards)\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Why This Matters in the Real World\n",
    "\n",
    "- üìú The **AI Act will soon affect most companies** deploying AI/ML in the EU\n",
    "- üè¢ Companies will need to **reference regulations, model cards, data sheets, and policies** ‚Äî often written in dense legal or technical language\n",
    "- üõ†Ô∏è A RAG-based assistant can:\n",
    "  - Help internal teams understand obligations faster\n",
    "  - Reduce compliance risk\n",
    "  - Make onboarding and documentation review easier\n",
    "  - Support fairness, explainability, and transparency goals\n",
    "\n",
    "---\n",
    "\n",
    "‚ú® By the end of this notebook, you‚Äôll have built a working AI assistant that can answer natural-language questions grounded in a real regulatory document ‚Äî a technique you can extend to many domains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85j-2BqQKQf9"
   },
   "source": [
    "üîß 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2KLVnQq4h2w4",
    "outputId": "8b9c5373-3a66-4d6d-d0b9-4dc16f74d981"
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain chromadb sentence-transformers pypdf unstructured\n",
    "!pip install -q transformers accelerate\n",
    "!pip install -q langchain-huggingface\n",
    "!pip install -q langchain-community\n",
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsUdf5_7Kowu"
   },
   "source": [
    "üìÑ 2. Download the EU AI Act PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OfRaf_lrif9t",
    "outputId": "ac4afe2e-1876-4977-ceed-ca5b4c94f04b"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "!wget -O eu_ai_act.pdf \"https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=OJ:L_202401689\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCET82L4LDhM"
   },
   "source": [
    "üìö 3. Load + Chunk the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dGkmcl5iy3B",
    "outputId": "dadb3bbf-5d22-49e1-8360-9d445bb14d42"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "import re\n",
    "\n",
    "loader = PyMuPDFLoader(\"eu_ai_act.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "docs = splitter.split_documents(pages)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Collapse multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "docs = [Document(page_content=clean_text(doc.page_content)) for doc in docs]\n",
    "\n",
    "print(f\"Loaded {len(docs)} document chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AucGpUhbLItz"
   },
   "source": [
    "üß† 4. Create Embeddings + Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7STeoEGZjT5E"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "db = Chroma.from_documents(docs, embedding)\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCkcoFbvLeek"
   },
   "source": [
    "ü§ñ 5.  Load **FLAN-T5-XL** for generating answers from retrieved document chunks\n",
    "\n",
    "üß† About the Language Model: FLAN-T5-XL\n",
    "\n",
    "This demo uses a powerful open-source model: [**FLAN-T5-XL**](https://huggingface.co/google/flan-t5-xl) from Google.\n",
    "\n",
    "It‚Äôs part of the **FLAN-T5** family ‚Äî models fine-tuned to follow instructions well.  \n",
    "We're using it here to answer questions about the EU AI Act using retrieved legal context.\n",
    "\n",
    "üì¶ Model Specs\n",
    "- **Name**: `google/flan-t5-xl`\n",
    "- **Size**: ~3 billion parameters\n",
    "- **Max Input Length**: 2048 tokens\n",
    "- **Strengths**:  \n",
    "  - Instruction following  \n",
    "  - Question answering  \n",
    "  - Summarization  \n",
    "  - Lightweight enough for Colab GPUs (if used carefully)\n",
    "\n",
    "üîó [View full model card on Hugging Face ‚Üí](https://huggingface.co/google/flan-t5-xl)\n",
    "\n",
    "---\n",
    "\n",
    "‚è≥ **Note**: The model may take **2 minutes to load** on the first run. That‚Äôs normal ‚Äî we‚Äôre pulling several GBs of weights and initializing it on the GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387,
     "referenced_widgets": [
      "c22d9c6692bc4d9b879f698a33badd5a",
      "a6c5c5e78cd14455b4519dfc15ce0422",
      "9073f03a205c4c35916e3bece37cd943",
      "37b8743af2cf496e8dd6a73c14d4ddb0",
      "a10501814c074734b9ec4cd80b765375",
      "9c26d1910ccf46f3956eb211e14963f7",
      "e3debb5805aa4152a4ab8b784e04111b",
      "78bf094e390a42218e27472041550918",
      "faa18a709608440a882f4916e99f6974",
      "f55ab397f1dd4fd68b420ff69bea61c1",
      "15aa126a71cb4df0b95dcf2a136a48be",
      "a89b8772e85b44a79f5cdf54ab35b12b",
      "11ed566e386e4d61973795bbc084c4d4",
      "f4d70af562ac46e6b70f96c323c9a619",
      "c4bf09b87b8c4b5484c72f29b6c9b756",
      "4eca4046856b40139bdf7045db28b2cf",
      "614e69c0f6ed411fb885d7077fe0d544",
      "4871352e41194ab79089de9dbd1df3aa",
      "cfbab1623fb64d78b2499df3e95ae5ef",
      "d8e96664d57d4b81b192b74a6ccd08d6",
      "231962dfea0840ef930c0eead513d394",
      "e6c3665e069240f4b540969d5809c123",
      "430ded527a5544de8a67bbf3e63b9536",
      "3c3a53a13ea247e9a5164c13d909719d",
      "2a8ff66a66304e37b149e95f48eb469d",
      "738fba35b8914aafbb1d68920585f921",
      "f5ea8a74de71454bb485e8fb7f2cc314",
      "bd4a38ea025a4563ac219dec8cde98e4",
      "fbe4fe78579f4bb9b44e736ec4afd577",
      "0f39579ebe3c4da88b28b11e43e3027f",
      "bc0ca2c890694a6d9ca0de1f5eb0e72b",
      "3b6872c619924e1c9d01d27eaba90dd5",
      "95f9fb829b7a420986075c1f90249558",
      "91b728572006410d86b6f8d845606d3e",
      "48c78703329e4671a34e6e849bb0d6f4",
      "54b130e56f91415babe09b1ca817ffc3",
      "effb5daa75134782bc6c1e07969802a6",
      "d9fbdce23f6d4d1aa3caaf843b99aeef",
      "1e7eaa82bd164ff2ae9db476ea85f15d",
      "241e8f63e6fa4e29be4e48766f9cf2e2",
      "06140396ad5a413289c3949a24f476ea",
      "c85549d031d547b8ab6834299297bc2b",
      "6c5a76747c6c453eac0c02f77ce4998b",
      "262cefb99d494c65b1e005a73f15bd11",
      "c6f4dd36aa8743d587b1f809c3c1735d",
      "b585469c225a4dea98f69d7fc1147c25",
      "bfbee91e8eea40feaabafdedf588aa2c",
      "eac3dd9f83e845d2a7f9a6af2a5d2bea",
      "6522b0a94a894160a44ce44bd998f9d3",
      "fce984d21c034a96a4a8280edd466f7e",
      "19289465adb34465b95d76eabc9626f0",
      "6e0890184f214896b4e08330b73ca517",
      "8c8445fc579f422ebf32d48fdbb10a43",
      "6fad3994cffc4229a62713f152fd1dfc",
      "f0d723b4d731471db9ce9c0bcea16a84",
      "87f5d773615640749408a3343912ee3d",
      "6deda1418bb84f85ae091eae7c0fa3ff",
      "c3e4bd635d1047478ac946489e2ff0cf",
      "2270207732b14ab3a28d75f4e52c3922",
      "4a6bc45a45804330b9d8bd4b8579b254",
      "fe631451c5a549558c2c3aaa9acbfc83",
      "fb48f763e03e4d64a013b109e9ce4dbf",
      "d04816472e4341b397337c60dc5bce86",
      "2cb4a7cebbb74bb8992ed8490cc42b7d",
      "8dde97c8a4154de49f097e6f7db684fb",
      "b7db474f2f28478e9cf26ed5a998e9a2",
      "ac5e801a275541f6b2bf1c14b87b2f1f",
      "af5cb97fdc834177ac482864a2ae315e",
      "205350791f954beab1bf3e2f2a01748e",
      "910dccaf4be049eda499a340bccd8868",
      "a36c950eabe547799a6e7fcd70cc0b15",
      "be24a1c1275143c693c6d0a636180657",
      "de4ca8388630481fa015bd08b0203917",
      "336f1d9125104efb960f76878c66e033",
      "9c62de55c37a42cd8baba23224a852fb",
      "ad1bbeabfcd74204b74194c215ae25c0",
      "f8ae8cb272534051a3658ec81d2e655d",
      "5958998cc7544e3d90025b28c755b06e",
      "bfc2882982d0467eb1fda7debbfd7490",
      "220e221fbe564289bcd5ac2e63914f85",
      "a75ea24b53d648dfa7a6a4008efcaf8c",
      "c2b9a4972b264cd5ab9295353134c9d7",
      "b8bdfde2080f4d84899a78e1a1ad8673",
      "c9d3237bb6a5480895098073d362555a",
      "a4ba4eaec6f04ad8976c2d0b44bd6018",
      "2d39fa7255394f9c8fa004beeae0413f",
      "25ad8b6e55a246ed8095b6f59f2f3d23",
      "10bf4d8a9d2c481e8421ec398413089f",
      "43777caa4c334f5bbd8f539eb866bba0",
      "0d7d712a46b9454fa69763c5adfb9d71",
      "f565d53b401d4de9bc0070e8a8b25868",
      "cdb467ecd0c145c5ae1ef6bea3cf12b6",
      "8f10ff14e1a4404a85f2ba81e6aba61c",
      "c5aa6c93b288444eb385872e2be70bad",
      "ead9f6792f014b669027df283c854a47",
      "7f9891321ec8437db285e5efca2180c2",
      "aa6259f4f6974436ae21cedc463a6dcb",
      "4e7e8a727b6b41cb8652f88829de487d",
      "d2b23a595e0e43c3b1450432419d0c03",
      "7de4bc08481e44c3b25d0d416cc2b5c7",
      "da3a17c26d2b44f592ac3710cbed69bb",
      "a10b90b438bb4ea3a0420da09379d897",
      "7e76a2a9fe634b2bb2d5bd099b25304d",
      "6b21c906d5ff407a87a21b5a7a1087f5",
      "86bb9b10688c478581f974ca2ae9769c",
      "4085dec68ba44c618c626c9da61a1f36",
      "5bdd181f7bf749e083abe76798697d43",
      "fb0cfb999a984174aa7d2513ddcda3a1",
      "ee35b4d50c0149ca8db96214c8ac5c3c",
      "54a07e96d27047d7baa27399f428d3d4",
      "f6ae2e3c5e0345bd90ee3c0476b37caf",
      "3070628ed78f49778a76efc7a7653c07",
      "15f71ef9ab504b669eeca932a47a701f",
      "4335fa4d4d554e039348621a64d930c7",
      "7ba0e4a6361f41faabe2a24d3c1dbd92",
      "407bb39da4a6403fb0ee45a515943b20",
      "853e8784823f45169b955a6b0b3d413a",
      "6e6ad130b2ca4cd2b531892b51fb14e9",
      "2254bf6083fd4e2c9d207a9adca7b206",
      "c8f30608cdf944e68b740f57a1030599",
      "6fa24234f7ae4b4a8894a96a6aa466aa"
     ]
    },
    "id": "F9jjz2FPj_zv",
    "outputId": "42d323e7-d1af-4080-d5a1-fdc7367e5e42"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "model_id = \"google/flan-t5-xl\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",         # Uses GPU automatically\n",
    "    torch_dtype=\"auto\"         # Enables fp16 if possible\n",
    ")\n",
    "\n",
    "flan_pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=flan_pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_I1_bim6L1Qb"
   },
   "source": [
    "üîÅ 6. Build RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKAOYm9ykEyt"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHc5TwhWL5Xs"
   },
   "source": [
    "üí¨ 7. Talk with AI.Act!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bP-7ZSD6WoBN",
    "outputId": "4ecd7a5a-1981-49f8-914c-bdda31525a78"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "while True:\n",
    "    query = input(\"Ask a question about the EU AI Act (or type 'exit'): \")\n",
    "    if query.lower() in ['exit', 'quit']:\n",
    "        break\n",
    "\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # print(\"\\nüîç Retrieved Context:\")\n",
    "    # for i, doc in enumerate(retrieved_docs):\n",
    "    #     print(f\"\\n--- Chunk {i+1} ---\\n{doc.page_content[:1000]}\")\n",
    "\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an expert assistant helping users understand the EU AI Act.\\n\"\n",
    "        \"Using only the context below, provide a clear and complete answer to the question.\\n\"\n",
    "        \"Summarize the answer in your own words. If needed, include key phrases or quotes.\\n\"\n",
    "        \"Do not copy full legal clauses verbatim unless specifically asked.\\n\"\n",
    "        \"If the context does not contain the answer, say 'Not found in the provided context.'\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    response = llm(prompt)\n",
    "    print(\"\\nüß† Answer:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
